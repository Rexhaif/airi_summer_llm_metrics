{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c58a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import AdamW, get_scheduler\n",
    "from transformers import pipeline\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from peft import (\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dd2bd9-77d8-48e8-aed3-06f4a7c4401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'bigscience/mt0-xxl-mt'\n",
    "HF_TOKEN = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab05736-d2ef-4a06-8a02-31e2107b90a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LORA_R = 8\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT= 0.05\n",
    "LORA_TARGET_MODULES = [\n",
    "    'q',\n",
    "    'v'\n",
    "]\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MICRO_BATCH_SIZE = 16\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LEARNING_RATE = 3e-4\n",
    "OUTPUT_DIR = 'mt0_finetune'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d0fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_train = load_dataset('RicardoRei/wmt-da-human-evaluation', split='train')\n",
    "wmt_test = load_dataset(\"RicardoRei/wmt-mqm-human-evaluation\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df92ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = ['en-de', 'en-ru', 'zh-en']\n",
    "id2langs = {\n",
    "    'ru': 'russian',\n",
    "    'en': 'english',\n",
    "    'de': 'german',\n",
    "    'zh': 'chinese'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4563521f-a69c-4120-816e-02182a197af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_test = wmt_test.filter(lambda example: (example['year'] == 2022) & (example['lp'] in translations))\n",
    "wmt_test = wmt_test.rename_column('score', 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df69213",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "train_take_part = 0.25\n",
    "eval_take_part = 0.01\n",
    "\n",
    "train_dataset = []\n",
    "eval_dataset = []\n",
    "\n",
    "for translation in translations:\n",
    "    train_part = wmt_train.filter(lambda example: (example['year'] != 2022) & (example['lp'] == translation))\n",
    "    train_part = train_part.shuffle(seed=seed)\n",
    "    train_dataset.append(Dataset.from_dict(train_part[:int(len(train_part)*train_take_part)]))\n",
    "    eval_dataset.append(Dataset.from_dict(train_part[int(len(train_part)*train_take_part):int(len(train_part)*(train_take_part+eval_take_part))]))\n",
    "train_dataset = concatenate_datasets(train_dataset)\n",
    "eval_dataset = concatenate_datasets(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e9006-c1b7-4b43-bc02-63aaf5e78a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    if text is None:\n",
    "        return ''\n",
    "    return ' '.join(text.lower().strip().split())\n",
    "\n",
    "def get_gemba_da_prompt(langs, source_seg, reference_seg, target_seg):\n",
    "    src_lang_id, tgt_lang_id = langs.split('-')\n",
    "    source_lang, target_lang = id2langs[src_lang_id], id2langs[tgt_lang_id]\n",
    "    source_seg, reference_seg, target_seg = map(preprocess, [source_seg, reference_seg, target_seg])\n",
    "    return f'Score the following translation from {source_lang} to {target_lang} respect to the human reference on a continuous scale from 0 to 100, ' \\\n",
    "            'where a score of zero means \"no meaning preserved\" and score of one hundred means \"perfect meaning and grammar\".\\n' \\\n",
    "            f'{source_lang} source: \"{source_seg}\"\\n' \\\n",
    "            f'{target_lang} human reference: \"{reference_seg}\"\\n' \\\n",
    "            f'{target_lang} translation: \"{target_seg}\"\\n' \\\n",
    "            'Score:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58d417f-6bb3-4b5c-b916-d315b6257f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=HF_TOKEN)\n",
    "tokenizer.pad_token = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f685e2-7414-4d90-bac6-1bd19f3aba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_(data, tokenizer, prompt_func, input_fields, target_field=None, max_length=MAX_LENGTH):\n",
    "    prompt = prompt_func(*[data[field] for field in input_fields])\n",
    "    result = tokenizer(prompt, truncation=True, max_length=max_length, padding=False)\n",
    "    if max_length is not None and result['input_ids'][-1] != tokenizer.eos_token_id \\\n",
    "        and len(result['input_ids']) < max_length:\n",
    "        result['input_ids'].append(tokenizer.eos_token_id)\n",
    "        result['attention_mask'].append(1)\n",
    "    if target_field is not None:\n",
    "        target_rokens = tokenizer(str(data[target_field]), truncation=True, max_length=max_length, padding=False)\n",
    "        result['labels'] = target_rokens['input_ids']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2260bd50-d0c7-4c50-90d5-722ffcd9d4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(partial(tokenize_,\n",
    "                                          tokenizer=tokenizer,\n",
    "                                          prompt_func=get_gemba_da_prompt,\n",
    "                                          input_fields=['lp', 'src', 'ref', 'mt'],\n",
    "                                          target_field='raw'), num_proc=4)\n",
    "eval_dataset = eval_dataset.map(partial(tokenize_,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        prompt_func=get_gemba_da_prompt,\n",
    "                                        input_fields=['lp', 'src', 'ref', 'mt'],\n",
    "                                        target_field='raw'), num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30336da-e083-4a6d-9d2a-c948840ad35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_datasets = DatasetDict({'train': train_dataset, 'eval': eval_dataset})\n",
    "finetune_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ead0d0-3758-4d84-b4ae-b8f7eb8474d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in finetune_datasets:\n",
    "    finetune_datasets[key] = finetune_datasets[key].remove_columns(\n",
    "        set(finetune_datasets[key].column_names)-{'input_ids', 'attention_mask', 'labels'}\n",
    "    )\n",
    "finetune_datasets.set_format('torch')\n",
    "finetune_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc0121c-8318-4219-95fd-4396c9bc1261",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b29df64-ce89-4aa4-8336-b54c5079bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    finetune_datasets['train'], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    finetune_datasets['eval'], batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8c1be-703c-49bb-bca3-f9a72efacd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4db760-635a-4bc7-99bd-bcd25aeebff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, device_map='auto', torch_dtype=torch.float16,\n",
    "                                              load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2404d24-e2f7-4a32-b16c-e32743e951ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e1cb1-49d7-414b-9006-3168ec50a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=MICRO_BATCH_SIZE,\n",
    "    logging_steps=20,\n",
    "    warmup_steps=100,\n",
    "    save_strategy='steps',\n",
    "    evaluation_strategy='steps',\n",
    "    weight_decay=1e-6,\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    dataloader_num_workers=4,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,\n",
    "    optim='adamw_torch'\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=finetune_datasets['train'],\n",
    "    eval_dataset=finetune_datasets['eval'],\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "old_state_dict = model.state_dict\n",
    "model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(\n",
    "        self, old_state_dict()\n",
    "    )\n",
    ").__get__(model, type(model))\n",
    "\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbf25ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a3d74e-5097-4a88-b6ef-0e8609991641",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./mt0-xxl-mt_wmt_finetune_lora_qvo_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9705a07a-b5a4-4e04-88a3-dc5958e0015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = wmt_test.map(partial(tokenize_,\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    prompt_func=get_gemba_da_prompt,\n",
    "                                    max_length=None,\n",
    "                                    input_fields=['lp', 'src', 'ref', 'mt'],\n",
    "                                    target_field='raw'),\n",
    "                            num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7513d9f-c953-4bfa-8ef0-7149f68eb5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = './mt0-xxl-mt_wmt_finetune_lora_qvo_final'\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "inference_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    config.base_model_name_or_path, device_map='auto', torch_dtype=torch.float16,\n",
    "    use_auth_token=HF_TOKEN, load_in_8bit=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, use_auth_token=HF_TOKEN)\n",
    "model = PeftModel.from_pretrained(inference_model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37274f7f-8d2a-4270-bdc7-d62ce8e1976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(text):\n",
    "    score = re.search('-?\\d+(\\.\\d+)?', text)\n",
    "    if not score:\n",
    "        return -1.0\n",
    "    score = score.group(0).split(' ', 1)[-1]\n",
    "    if score.replace('.', '').isnumeric:\n",
    "        return float(score)\n",
    "    return -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a115cac-fda9-48e7-b466-c45d42c57861",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "preds = defaultdict(list)\n",
    "labels = defaultdict(list)\n",
    "for translation in translations:\n",
    "    print(translation)\n",
    "    lang_dataset = test_dataset.filter(lambda x: x['lp'] == translation)\n",
    "    for k, v in zip(lang_dataset['lp'], lang_dataset['raw']):\n",
    "        labels[k].append(v)\n",
    "    lang_dataset = lang_dataset.remove_columns(set(lang_dataset.column_names)-{'input_ids', 'attention_mask', 'labels'})\n",
    "    lang_dataset.set_format('torch')\n",
    "    lang_dataloader = DataLoader(\n",
    "        lang_dataset, shuffle=False, batch_size=32, collate_fn=data_collator\n",
    "    )\n",
    "    pb = tqdm_notebook(lang_dataloader)\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            for batch in pb:\n",
    "                labels_ = batch.pop('labels')\n",
    "                labels_ = np.where(labels_ != -100, labels_, tokenizer.pad_token_id)\n",
    "\n",
    "                try:\n",
    "                    decoded_preds = tokenizer.batch_decode(model.generate(input_ids=batch['input_ids'].to('cuda'), do_sample=True, top_k=5, max_new_tokens=10), skip_special_tokens=True)\n",
    "                except RuntimeError:\n",
    "                    decoded_preds = ['-1.0'] * len(batch)\n",
    "                decoded_labels = tokenizer.batch_decode(labels_, skip_special_tokens=True)\n",
    "\n",
    "                for pred, label in zip(decoded_preds, decoded_labels):\n",
    "                    label = get_score(label)\n",
    "                    if label < 0:\n",
    "                        continue\n",
    "                    pred = get_score(pred)\n",
    "                    preds[translation].append(pred)\n",
    "                    labels[translation].append(label)\n",
    "\n",
    "                tau = kendalltau(preds[translation], labels[translation][:len(preds[translation])])\n",
    "                pb.set_description(f\"'tau': {tau.statistic}\")\n",
    "                torch.cuda.empty_cache()\n",
    "    except KeyboardInterrupt:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fd84bc-4e91-4c03-aacd-196e6ee6746c",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
