{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c58a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import Trainer, TrainingArguments, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import AdamW, get_scheduler\n",
    "from transformers import pipeline\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb6e3b-99b3-4c6f-9272-f0efda41e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'meta-llama/Llama-2-7b-hf'\n",
    "HF_TOKEN = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab05736-d2ef-4a06-8a02-31e2107b90a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT= 0.05\n",
    "LORA_TARGET_MODULES = [\n",
    "    'q_proj',\n",
    "    'k_proj',\n",
    "    'v_proj',\n",
    "    'up_proj',\n",
    "    'down_proj'\n",
    "]\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MICRO_BATCH_SIZE = 32\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LEARNING_RATE = 3e-4\n",
    "OUTPUT_DIR = \"llama2_7b_finetune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d0fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_train = load_dataset('RicardoRei/wmt-da-human-evaluation', split='train')\n",
    "wmt_test = load_dataset(\"RicardoRei/wmt-mqm-human-evaluation\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df92ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = ['en-de', 'en-ru', 'zh-en']\n",
    "id2langs = {\n",
    "    'ru': 'russian',\n",
    "    'en': 'english',\n",
    "    'de': 'german',\n",
    "    'zh': 'chinese'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656aefc7-c0d5-420e-9866-2cce7aac8eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_test = wmt_test.filter(lambda example: (example['year'] == 2022) & (example['lp'] in translations))\n",
    "wmt_test = wmt_test.rename_column('score', 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df69213",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "train_take_part = 0.25\n",
    "eval_take_part = 0.01\n",
    "\n",
    "train_dataset = []\n",
    "eval_dataset = []\n",
    "\n",
    "for translation in translations:\n",
    "    train_part = wmt_train.filter(lambda example: (example['year'] != 2022) & (example['lp'] == translation))\n",
    "    train_part = train_part.shuffle(seed=seed)\n",
    "    train_dataset.append(Dataset.from_dict(train_part[:int(len(train_part)*train_take_part)]))\n",
    "    eval_dataset.append(Dataset.from_dict(train_part[int(len(train_part)*train_take_part):int(len(train_part)*(train_take_part+eval_take_part))]))\n",
    "train_dataset = concatenate_datasets(train_dataset)\n",
    "eval_dataset = concatenate_datasets(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e9006-c1b7-4b43-bc02-63aaf5e78a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    if text is None:\n",
    "        return ''\n",
    "    return ' '.join(text.lower().strip().split())\n",
    "\n",
    "def get_gemba_da_prompt(langs, source_seg, reference_seg, target_seg, score=None):\n",
    "    src_lang_id, tgt_lang_id = langs.split('-')\n",
    "    source_lang, target_lang = id2langs[src_lang_id], id2langs[tgt_lang_id]\n",
    "    source_seg, reference_seg, target_seg = map(preprocess, [source_seg, reference_seg, target_seg])\n",
    "    return ''.join([f'Score the following translation from {source_lang} to {target_lang} respect to the human reference on a continuous scale from 0 to 100, ',\n",
    "            'where a score of zero means \"no meaning preserved\" and score of one hundred means \"perfect meaning and grammar\".\\n',\n",
    "            f'{source_lang} source: \"{source_seg}\"\\n',\n",
    "            f'{target_lang} human reference: \"{reference_seg}\"\\n',\n",
    "            f'{target_lang} translation: \"{target_seg}\"\\n',\n",
    "            'Score:' if score is None else f'Score: {score}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e126e47-ea0f-4cb3-bf72-b5d808b638ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_datasets = DatasetDict({'train': train_dataset, 'eval': eval_dataset})\n",
    "wmt_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58d417f-6bb3-4b5c-b916-d315b6257f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama2_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=HF_TOKEN)\n",
    "llama2_tokenizer.pad_token = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f685e2-7414-4d90-bac6-1bd19f3aba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_translation(data, tokenizer, prompt_func, input_fields, target_field=None, max_length=MAX_LENGTH, inference=False):\n",
    "    prompt = prompt_func(*[data[field] for field in input_fields])\n",
    "    result = tokenizer(prompt, truncation=True, max_length=max_length, padding=False)\n",
    "    if result['input_ids'][-1] != tokenizer.eos_token_id \\\n",
    "        and len(result['input_ids']) < max_length:\n",
    "        result['input_ids'].append(tokenizer.eos_token_id)\n",
    "        result['attention_mask'].append(1)\n",
    "    if not inference:\n",
    "        result['labels'] = result['input_ids'].copy()\n",
    "    else:\n",
    "        prompt = prompt_func(*[data[field] for field in input_fields+[target_field]])\n",
    "        target_rokens = tokenizer(prompt, truncation=True, max_length=max_length, padding=False)\n",
    "        result['labels'] = target_rokens['input_ids'].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1989a9d-4055-4af5-a7b6-99f53c238016",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama2_wmt_finetune_datasets = wmt_datasets.map(partial(tokenize_translation,\n",
    "                                                        tokenizer=llama2_tokenizer,\n",
    "                                                        prompt_func=get_gemba_da_prompt,\n",
    "                                                        input_fields=['lp', 'src', 'ref', 'mt', 'raw']),\n",
    "                                                num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ead0d0-3758-4d84-b4ae-b8f7eb8474d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama2_wmt_finetune_datasets = llama2_wmt_finetune_datasets.remove_columns(set(llama2_wmt_finetune_datasets['train'].column_names)-{'input_ids', 'attention_mask', 'labels'})\n",
    "llama2_wmt_finetune_datasets.set_format('torch')\n",
    "llama2_wmt_finetune_datasets['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc0121c-8318-4219-95fd-4396c9bc1261",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    llama2_tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b29df64-ce89-4aa4-8336-b54c5079bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    llama2_wmt_finetune_datasets['train'], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    llama2_wmt_finetune_datasets['eval'], batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8c1be-703c-49bb-bca3-f9a72efacd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4db760-635a-4bc7-99bd-bcd25aeebff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map='auto', use_auth_token=HF_TOKEN,\n",
    "                                             torch_dtype=torch.float16, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2404d24-e2f7-4a32-b16c-e32743e951ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025e5cc-5182-44b0-b681-fc680797fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(text):\n",
    "    score = re.search('Score: -?\\d+(\\.\\d+)?', text)\n",
    "    if not score:\n",
    "        return -1.0\n",
    "    score = score.group(0).split(' ', 1)[-1]\n",
    "    if score.replace('.', '').isnumeric:\n",
    "        return float(score)\n",
    "    return -1.0\n",
    "\n",
    "def compute_metrics(eval_data, tokenizer):\n",
    "    preds, labels = eval_data\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    pred_scores = []\n",
    "    label_scores = []\n",
    "    skips = 0\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        label = get_score(label)\n",
    "        if label < 0:\n",
    "            skips += 1\n",
    "            continue\n",
    "        pred = get_score(pred)\n",
    "        pred_scores.append(pred)\n",
    "        label_scores.append(label)\n",
    "\n",
    "    r = spearmanr(pred_scores, label_scores)\n",
    "    tau = kendalltau(pred_scores, label_scores)\n",
    "    return {'R': r.statistic, 'tau': tau.statistic, 'skip_for_eval': skips}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e1cb1-49d7-414b-9006-3168ec50a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./llama2_7b_finetune\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=MICRO_BATCH_SIZE,\n",
    "    logging_steps=20,\n",
    "    warmup_steps=100,\n",
    "    save_strategy='steps',\n",
    "    evaluation_strategy='steps',\n",
    "    weight_decay=1e-6,\n",
    "    eval_steps=400,\n",
    "    save_steps=400,\n",
    "    save_total_limit=3,\n",
    "    dataloader_num_workers=4,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,\n",
    "    optim='adamw_torch',\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_LENGTH+10\n",
    ")\n",
    "\n",
    "trainer = transformers.Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=llama2_tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=llama2_wmt_finetune_datasets['train'],\n",
    "    eval_dataset=llama2_wmt_finetune_datasets['eval'],\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=partial(compute_metrics, tokenizer=llama2_tokenizer)\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "old_state_dict = model.state_dict\n",
    "model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(\n",
    "        self, old_state_dict()\n",
    "    )\n",
    ").__get__(model, type(model))\n",
    " \n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbf25ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a3d74e-5097-4a88-b6ef-0e8609991641",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./llama2_7b_wmt_finetune_lora_qvproj_final/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9705a07a-b5a4-4e04-88a3-dc5958e0015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = wmt_test.map(partial(tokenize_translation,\n",
    "                                    tokenizer=llama2_tokenizer,\n",
    "                                    prompt_func=get_gemba_da_prompt,\n",
    "                                    max_length=MAX_LENGTH,\n",
    "                                    input_fields=['lp', 'src', 'ref', 'mt'],\n",
    "                                    target_field='raw',\n",
    "                                    inference=True),\n",
    "                            num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a85422f-7ba1-4b5d-99a2-85d67be7d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = './llama2_7b_wmt_finetune_lora_qvproj_process/checkpoint-900/'\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "inference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path, device_map='auto', torch_dtype=torch.float16,\n",
    "    use_auth_token=HF_TOKEN, load_in_8bit=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, use_auth_token=HF_TOKEN)\n",
    "model = PeftModel.from_pretrained(inference_model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a9de9-1bd2-447a-8a45-26f2ad785af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "preds = defaultdict(list)\n",
    "labels = defaultdict(list)\n",
    "for translation in translations:\n",
    "    print(translation)\n",
    "    lang_dataset = test_dataset.filter(lambda x: x['lp'] == translation)\n",
    "    for k, v in zip(lang_dataset['lp'], lang_dataset['raw']):\n",
    "        labels[k].append(v)\n",
    "    lang_dataset = lang_dataset.remove_columns(set(lang_dataset.column_names)-{'input_ids', 'attention_mask', 'labels'})\n",
    "    lang_dataset.set_format('torch')\n",
    "    lang_dataloader = DataLoader(\n",
    "        lang_dataset, shuffle=False, batch_size=16, collate_fn=data_collator\n",
    "    )\n",
    "    pb = tqdm_notebook(lang_dataloader)\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            for batch in pb:\n",
    "                labels_ = batch.pop('labels')\n",
    "                labels_ = np.where(labels_ != -100, labels_, llama2_tokenizer.pad_token_id)\n",
    "\n",
    "                try:\n",
    "                    decoded_preds = llama2_tokenizer.batch_decode(model.generate(input_ids=batch['input_ids'].to('cuda'), do_sample=True, top_k=5, max_new_tokens=10), skip_special_tokens=True)\n",
    "                except RuntimeError:\n",
    "                    decoded_preds = ['-1.0'] * len(batch)\n",
    "                decoded_labels = llama2_tokenizer.batch_decode(labels_, skip_special_tokens=True)\n",
    "\n",
    "                for pred, label in zip(decoded_preds, decoded_labels):\n",
    "                    label = get_score(label)\n",
    "                    if label < 0:\n",
    "                        continue\n",
    "                    pred = get_score(pred)\n",
    "                    preds[translation].append(pred)\n",
    "                    labels[translation].append(label)\n",
    "\n",
    "                tau = kendalltau(preds[translation], labels[translation][:len(preds[translation])])\n",
    "                pb.set_description(f\"'tau': {tau.statistic}\")\n",
    "                torch.cuda.empty_cache()\n",
    "    except KeyboardInterrupt:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72adf557-d6ea-41ca-8d28-4fcd40476f6c",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
