{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c58a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import Trainer, TrainingArguments, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import AdamW, get_scheduler\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "from functools import partial\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab05736-d2ef-4a06-8a02-31e2107b90a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LORA_R = 32\n",
    "LORA_ALPHA = 8\n",
    "LORA_DROPOUT= 0.05\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MICRO_BATCH_SIZE = 16\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LEARNING_RATE = 3e-4\n",
    "OUTPUT_DIR = \"llama2_7b_finetune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d0fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_base = load_dataset('RicardoRei/wmt-da-human-evaluation', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df92ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = ['en-de', 'en-ru', 'zh-en']\n",
    "id2langs = {\n",
    "    'ru': 'russian',\n",
    "    'en': 'english',\n",
    "    'de': 'german',\n",
    "    'zh': 'chinese'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f656224-be2b-4861-bc77-91d9f99ed978",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = wmt_base.filter(lambda example: (example['year'] == 2022) & (example['lp'] in translations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df69213",
   "metadata": {},
   "outputs": [],
   "source": [
    "take_part = 0.5\n",
    "train_dataset = []\n",
    "eval_dataset = []\n",
    "for translation in translations:\n",
    "    train_part = wmt_base.filter(lambda example: (example['year'] != 2022) & (example['lp'] == translation))\n",
    "    train_part = train_part.shuffle(seed=42)\n",
    "    train_dataset.append(Dataset.from_dict(train_part[:int(len(train_part)*take_part)]))\n",
    "\n",
    "    eval_part = wmt_base.filter(lambda example: (example['year'] == 2022) & (example['lp'] == translation))\n",
    "    eval_part = eval_part.shuffle(seed=42)\n",
    "    eval_dataset.append(Dataset.from_dict(eval_part[:int(len(eval_part)*take_part)]))\n",
    "train_dataset = concatenate_datasets(train_dataset)\n",
    "eval_dataset = concatenate_datasets(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e9006-c1b7-4b43-bc02-63aaf5e78a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gemba_da_prompt(langs, source_seg, reference_seg, target_seg, score=None):\n",
    "    source_lang, target_lang = langs.split('-')\n",
    "    return ''.join([f'Score the following translation from {source_lang} to {target_lang} respect to the human reference on a continuous scale from 0 to 100, ',\n",
    "            'where a score of zero means \"no meaning preserved\" and score of one hundred means \"perfect meaning and grammar\".\\n',\n",
    "            f'{source_lang} source: \"{source_seg}\"\\n',\n",
    "            f'{target_lang} human reference: \"{reference_seg}\"\\n',\n",
    "            f'{target_lang} translation: \"{target_seg}\"\\n',\n",
    "            'Score:' if score is None else f'Score: {score}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e126e47-ea0f-4cb3-bf72-b5d808b638ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_datasets = DatasetDict({'train': train_dataset, 'eval': eval_dataset})\n",
    "wmt_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58d417f-6bb3-4b5c-b916-d315b6257f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama2_tokenizer = AutoTokenizer.from_pretrained('./llama-2-7b-hf/')\n",
    "llama2_tokenizer.pad_token = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f685e2-7414-4d90-bac6-1bd19f3aba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_translation(data, tokenizer, prompt_func, fields, max_length=MAX_LENGTH):\n",
    "    prompt = prompt_func(*[data[field] for field in fields])\n",
    "    result = tokenizer(prompt, truncation=True, max_length=max_length, padding=False)\n",
    "    if result['input_ids'][-1] != tokenizer.eos_token_id \\\n",
    "        and len(result['input_ids']) < max_length:\n",
    "        result['input_ids'].append(tokenizer.eos_token_id)\n",
    "        result['attention_mask'].append(1)\n",
    "    result['labels'] = result['input_ids'].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1989a9d-4055-4af5-a7b6-99f53c238016",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama2_wmt_finetune_datasets = wmt_datasets.map(partial(tokenize_translation,\n",
    "                                                        tokenizer=llama2_tokenizer,\n",
    "                                                        prompt_func=get_gemba_da_prompt,\n",
    "                                                        fields=['lp', 'src', 'ref', 'mt', 'raw']),\n",
    "                                                num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ead0d0-3758-4d84-b4ae-b8f7eb8474d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama2_wmt_finetune_datasets = llama2_wmt_finetune_datasets.remove_columns(set(llama2_wmt_finetune_datasets['train'].column_names)-{'input_ids', 'attention_mask', 'labels'})\n",
    "llama2_wmt_finetune_datasets.set_format('torch')\n",
    "llama2_wmt_finetune_datasets['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc0121c-8318-4219-95fd-4396c9bc1261",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    llama2_tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b29df64-ce89-4aa4-8336-b54c5079bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    llama2_wmt_finetune_datasets['train'], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    llama2_wmt_finetune_datasets['eval'], batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8c1be-703c-49bb-bca3-f9a72efacd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4db760-635a-4bc7-99bd-bcd25aeebff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('./llama-2-7b-hf/', device_map='auto',\n",
    "                                             torch_dtype=torch.float16, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2404d24-e2f7-4a32-b16c-e32743e951ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025e5cc-5182-44b0-b681-fc680797fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_data, tokenizer):\n",
    "    preds, labels = eval_data\n",
    "    print\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    pred_scores = []\n",
    "    label_scores = []\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        label = label.rsplit('Score:', 1)[-1].strip()\n",
    "        if not label.replace('.', '').isnumeric():\n",
    "            continue\n",
    "        pred = pred.rsplit('Score:', 1)[-1].strip()\n",
    "        pred = float(pred) if pred.replace('.', '').isnumeric() else -1.0\n",
    "        pred_scores.append(float(pred))\n",
    "        label_scores.append(float(label))\n",
    "\n",
    "    r = spearmanr(pred_scores, label_scores)\n",
    "    tau = kendalltau(pred_scores, label_scores)\n",
    "    return {'R': r.statistic, 'tau': tau.statistic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e1cb1-49d7-414b-9006-3168ec50a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./llama2_7b_finetune\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=MICRO_BATCH_SIZE//2,\n",
    "    logging_steps=5,\n",
    "    warmup_steps=100,\n",
    "    save_strategy='steps',\n",
    "    evaluation_strategy='steps',\n",
    "    weight_decay=1e-6,\n",
    "    eval_steps=5,\n",
    "    save_steps=5,\n",
    "    save_total_limit=3,\n",
    "    dataloader_num_workers=4,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,\n",
    "    optim='adamw_torch',\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_LENGTH+1\n",
    ")\n",
    "\n",
    "trainer = transformers.Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=llama2_tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=llama2_wmt_finetune_datasets['train'],\n",
    "    eval_dataset=llama2_wmt_finetune_datasets['eval'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=partial(compute_metrics, tokenizer=llama2_tokenizer)\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "old_state_dict = model.state_dict\n",
    "model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(\n",
    "        self, old_state_dict()\n",
    "    )\n",
    ").__get__(model, type(model))\n",
    " \n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbf25ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f10afcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
