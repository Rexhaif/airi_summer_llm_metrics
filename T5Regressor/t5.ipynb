{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717b8293-c172-4128-b543-cf53d77f51e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-24 14:31:01,985] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from datasets import Dataset as ds\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    MT5EncoderModel,\n",
    "    get_scheduler,\n",
    ")\n",
    "from transformers.modeling_outputs import BaseModelOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f1354a-a135-4592-b514-b6794057be2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.447509765625"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.mem_get_info()[0] / 1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66414830-7d03-42d9-b24b-0720d2ff1915",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_encoder_name = \"bigscience/mt0-base\"\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f32b23da-d048-40ff-a42d-4f5ca18b81c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33megoluback\u001b[0m (\u001b[33mhse_image_captioning_spring_project\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963b6e2-d664-4553-86e1-b82f583ef862",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a05193-2060-4d0d-86b0-5ec3a5478934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/RicardoRei___csv/RicardoRei--wmt-da-human-evaluation-a4a96cd6106c3667/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"RicardoRei/wmt-da-human-evaluation\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7decec72-12a6-4dc8-ac4c-bd2d81e4f9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/RicardoRei___csv/RicardoRei--wmt-da-human-evaluation-a4a96cd6106c3667/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-119fc26d246c3506.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset_train = dataset.filter(\n",
    "    lambda example: (example[\"year\"] != 2022)\n",
    "    and (\n",
    "        (example[\"lp\"] == \"en-ru\")\n",
    "        or (example[\"lp\"] == \"zh-en\")\n",
    "        or (example[\"lp\"] == \"en-de\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "936c2aae-c173-4ba6-be4a-0dcfd1be5d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361129/361129 [00:58<00:00, 6213.37it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt_column = []\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Score the following translation from {source_lang} to {target_lang} with respect to the human reference on a continuous scale from 0 to 100, where score of zero means \"no meaning preserved\" and score of one hundred means \"perfect meaning and grammar\".\n",
    "{source_lang} source: \"{source_seg}\"\n",
    "{target_lang} human reference: {reference_seg}\n",
    "{target_lang} translation: \"{target_seg}\"\n",
    "Score:\n",
    "\"\"\"\n",
    "\n",
    "for i in tqdm(range(len(dataset_train))):\n",
    "    example = dataset_train[i]\n",
    "    sl, tl = example[\"lp\"].split(\"-\")\n",
    "    prompt_column.append(\n",
    "        prompt_template.format(\n",
    "            source_lang=sl,\n",
    "            target_lang=tl,\n",
    "            source_seg=example[\"src\"],\n",
    "            reference_seg=example[\"ref\"],\n",
    "            target_seg=example[\"mt\"],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b35dea71-4688-4cd3-b8ad-1e73722640c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/RicardoRei___csv/RicardoRei--wmt-da-human-evaluation-a4a96cd6106c3667/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-b815f5b3ffcfbe9a.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset_train = dataset_train.add_column(name=\"prompt\", column=prompt_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6244e97-b50c-4ef2-9a1b-f56e4b86b903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lp': 'zh-en',\n",
       " 'src': '在三年半后重新任职总统之前，普京先生担任俄罗斯总理一职。',\n",
       " 'mt': 'Mr. Putin served as Russian prime minister before resuming his presidency Prime Minister of Russia after three and a half years in office.',\n",
       " 'ref': 'Mr Putin became prime minister, before returning to the presidency just three-and-a-half years later.',\n",
       " 'score': -0.048723632415222,\n",
       " 'raw': 74.5,\n",
       " 'annotators': 2,\n",
       " 'domain': 'news',\n",
       " 'year': 2017,\n",
       " 'prompt': '\\nScore the following translation from zh to en with respect to the human reference on a continuous scale from 0 to 100, where score of zero means \"no meaning preserved\" and score of one hundred means \"perfect meaning and grammar\".\\nzh source: \"在三年半后重新任职总统之前，普京先生担任俄罗斯总理一职。\"\\nen human reference: Mr Putin became prime minister, before returning to the presidency just three-and-a-half years later.\\nen translation: \"Mr. Putin served as Russian prime minister before resuming his presidency Prime Minister of Russia after three and a half years in office.\"\\nScore:\\n'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[41152]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ab533-9025-454c-a7d7-b60823c3c9de",
   "metadata": {
    "tags": []
   },
   "source": [
    "# T5 tokenizer initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d32a185e-31c3-4413-aac9-d34de30fb5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_encoder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3915b451-8708-443f-95b9-e81ab73451b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e69c57a-423e-43cf-82b2-ba83541264ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe70aa73-b4ee-4617-ba0f-f8ee67eb8b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/361129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_tokenized = dataset_train.map(\n",
    "    lambda example: tokenizer(\n",
    "        example[\"prompt\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    ),\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a55af3a8-c304-4260-b677-4d7d846419ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/361129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_tokenized.save_to_disk(\"wmt-da_tokenized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f017e0d-d09a-4d40-8691-4da5da170ce9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Convert to DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1eb6676-04a0-4c74-a902-6eb5c9819436",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized = load_from_disk(\"wmt-da_tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b81416e-3658-472c-b3c2-66b5d4663ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized = ds.from_dict(dataset_tokenized[:25000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ff2d51a-85c6-4d93-b2f9-f751171b43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized = (\n",
    "    dataset_tokenized.with_format(\"torch\")\n",
    "    .remove_columns(\n",
    "        [\"lp\", \"src\", \"mt\", \"ref\", \"score\", \"annotators\", \"domain\", \"year\", \"prompt\"]\n",
    "    )\n",
    "    .rename_column(\"raw\", \"label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff79357f-56ab-4973-bd72-4da602264907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_traineval = dataset_tokenized.train_test_split(test_size=0.2, seed=random_seed)\n",
    "\n",
    "dataset_traineval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "196bc470-2dfa-4397-adb8-6081d8634fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collactor = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8efeb0ce-f37e-4a8c-ba78-08b36aaab853",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(\n",
    "    dataset_traineval[\"train\"], batch_size=8, shuffle=True, collate_fn=data_collactor\n",
    ")\n",
    "\n",
    "dataloader_eval = DataLoader(\n",
    "    dataset_traineval[\"test\"], batch_size=8, collate_fn=data_collactor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce3f1b-0cb6-4f80-b143-597b59951b1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d950bd87-a4f2-4499-a8c6-a05c2e2cd8ba",
   "metadata": {},
   "source": [
    "To check:\n",
    "\n",
    "1. Different activations on last layer(e.g. Sigmoid)\n",
    "2. Different activations on hidden layers\n",
    "3. Different losses(e.g. RMSE)\n",
    "4. Different hidden layers in MLP\n",
    "5. More/less dropouts\n",
    "6. Different batch size\n",
    "7. Differen lr\n",
    "\n",
    "To do:\n",
    "1. Move .to(device) from train loop to tokenize\n",
    "2. Use LoRA adapter\n",
    "3. Use int8/int4\n",
    "4. Replace quality metric with Kendal-tau and Spearman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdbcc0d-f46b-43f3-ac1a-cfb0fa0722b9",
   "metadata": {},
   "source": [
    "Plan:\n",
    "\n",
    "Add WandB and check train/loss function on big train dataset and base architecture(t5+dropout+mlp without dropouts, mse loss, no act on last layer) <br />\n",
    "Do other experiments whether it converges or it doesn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "289fe75e-8e0f-4ada-8826-0518d3b17daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    )\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "        input_mask_expanded.sum(1), min=1e-9\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6d58d1c-58d0-4f47-b81e-419cc7a80e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Regressor(nn.Module):\n",
    "    def __init__(self, checkpoint, sizes_mlp, act=nn.ReLU):\n",
    "        super(T5Regressor, self).__init__()\n",
    "\n",
    "        self.llm = MT5EncoderModel.from_pretrained(\n",
    "            checkpoint, output_attentions=True, output_hidden_states=True\n",
    "        )\n",
    "\n",
    "        self.llm_output_shape = sizes_mlp[0]\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.layers = []\n",
    "        for i in range(len(sizes_mlp) - 1):\n",
    "            self.layers.append(nn.Linear(sizes_mlp[i], sizes_mlp[i + 1]))\n",
    "            if i < len(sizes_mlp) - 2:\n",
    "                self.layers.append(act())\n",
    "\n",
    "        self.mlp = nn.Sequential(*self.layers)\n",
    "        # self.output_layer = nn.Sigmoid()\n",
    "\n",
    "        self.loss_fc = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        outputs = self.llm(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # print(outputs.attentions[-1][:, 0, :, 0].size())\n",
    "        embeddings = mean_pooling(\n",
    "            outputs.last_hidden_state, outputs.attentions[-1][:, 0, :, 0]\n",
    "        )\n",
    "        outputs_sequence = self.dropout(embeddings)\n",
    "        # print(outputs_sequence.size())\n",
    "\n",
    "        # logits = (\n",
    "        #     self.output_layer(\n",
    "        #         # self.mlp(outputs_sequence[:, 0, :].view(-1, self.llm_output_shape))\n",
    "        #         self.mlp(outputs_sequence)\n",
    "        #     )\n",
    "        #     * 100\n",
    "        # )\n",
    "        logits = self.mlp(outputs_sequence)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fc(logits.view(-1, 1), labels.view(-1).unsqueeze(1))\n",
    "\n",
    "        return (\n",
    "            BaseModelOutput(\n",
    "                last_hidden_state=outputs.last_hidden_state,\n",
    "                hidden_states=outputs.hidden_states,\n",
    "                attentions=outputs.attentions,\n",
    "            ),\n",
    "            logits,\n",
    "            loss,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1147f-6b2f-4410-b956-7b4ca091724c",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e009fda6-ee20-4a71-8ea9-a6658c8c62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(entity=\"airi23-efficient-llm-metrics\", project=\"t5regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be482055-dfc6-4dc8-b2e3-b7b31bd85a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Regressor(\n",
       "  (llm): MT5EncoderModel(\n",
       "    (shared): Embedding(250112, 768)\n",
       "    (encoder): MT5Stack(\n",
       "      (embed_tokens): Embedding(250112, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): MT5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): MT5LayerSelfAttention(\n",
       "              (SelfAttention): MT5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): MT5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): MT5LayerFF(\n",
       "              (DenseReluDense): MT5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): MT5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-11): 11 x MT5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): MT5LayerSelfAttention(\n",
       "              (SelfAttention): MT5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): MT5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): MT5LayerFF(\n",
       "              (DenseReluDense): MT5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): MT5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): MT5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=192, out_features=48, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=48, out_features=1, bias=True)\n",
       "  )\n",
       "  (loss_fc): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = T5Regressor(checkpoint=model_encoder_name, sizes_mlp=[768, 192, 48, 1])\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58b1ca04-bc0e-4201-817f-41c21dfa135b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0676],\n",
       "        [0.0690],\n",
       "        [0.0684],\n",
       "        [0.0686],\n",
       "        [0.0683],\n",
       "        [0.0675],\n",
       "        [0.0682],\n",
       "        [0.0722]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = {k: v.to(device) for k, v in next(iter(dataloader_train)).items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "\n",
    "outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "405b1f16-f2dc-4b24-bb19-a504d029ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a67c98a-4416-4cd5-a02f-bae96af0b839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(dataloader_train)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11440eef-fd08-423a-bf1b-873899df621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = nn.MSELoss()\n",
    "# TODO: replace with Kendall-tau/Spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa5e4f89-8a58-4282-9f21-92e7841ccfdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2500 [00:00<?, ?it/s]\n",
      "  0%|          | 0/625 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2500\n",
      "Eval size: 625\n",
      "TRAIN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 1146/2500 [05:39<06:41,  3.38it/s, loss=424, logits=tensor([62.1954], device='cuda:0', grad_fn=<SelectBackward0>)]    "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# loss\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "progress_bar_train = tqdm(range(num_training_steps))\n",
    "progress_bar_eval = tqdm(range(num_epochs * len(dataloader_eval)))\n",
    "\n",
    "print(f\"Train size: {len(dataloader_train)}\")\n",
    "print(f\"Eval size: {len(dataloader_eval)}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"TRAIN\")\n",
    "    model.train()\n",
    "    for batch in dataloader_train:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss = outputs[2]  # loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        # progress_bar_train.set_postfix({\"loss\": loss.item()})\n",
    "        progress_bar_train.set_postfix({\"loss\": loss.item(), \"logits\": outputs[1][1]})\n",
    "        progress_bar_train.update(1)\n",
    "\n",
    "        # wandb.log({\"loss\": loss.item()})\n",
    "\n",
    "    print(\"EVAL\")\n",
    "    model.eval()\n",
    "    mse_metrics = []\n",
    "    for batch in dataloader_eval:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs[1]\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        mse_metric = metric(predictions, batch[\"labels\"]).item()\n",
    "        mse_metrics.append(mse_metric)\n",
    "        progress_bar_eval.set_postfix({\"loss\": mse_metric})\n",
    "        progress_bar_eval.update(1)\n",
    "\n",
    "    print(f\"Eval MSE: {mean(mse_metrics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f795c9-6d27-43cd-b2fe-8dc01e332ff9",
   "metadata": {},
   "source": [
    "To do:\n",
    "figure out why mse metric is constant on all eval epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "814a4033-da1d-4281-8351-479174751a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[17763,     8,   826,  ...,     0,     0,     0],\n",
       "         [17763,     8,   826,  ...,     0,     0,     0],\n",
       "         [17763,     8,   826,  ...,     0,     0,     0],\n",
       "         [17763,     8,   826,  ...,     0,     0,     0]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'),\n",
       " 'labels': tensor([43., 90., 82., 43.], device='cuda:0')}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  }
 ],
 "metadata": {
  "@deathbeds/ipydrawio": {
   "xml": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
